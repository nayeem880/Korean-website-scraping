{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "institutional-destiny",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://class101.net/creative\n",
      "https://class101.net/money\n",
      "https://class101.net/career\n",
      "https://class101.net/kids\n",
      "https://class101.net/signature\n"
     ]
    }
   ],
   "source": [
    "# importing the required modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "base = \"https://class101.net\"\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "urls = pd.read_csv('parent_links.csv')\n",
    "urls = urls.drop(['Unnamed: 0'], axis = 1) \n",
    "\n",
    "b = list(urls['0'])\n",
    "b = b[:5]\n",
    "a = []\n",
    "\n",
    "all_links = []\n",
    "l_links = []\n",
    "\n",
    "\n",
    "for i in range(len(b)):\n",
    "    url = b[i]\n",
    "    print(url)\n",
    "    \n",
    "    ##########defininf function\n",
    "    def scroll(url):\n",
    "        count = 0\n",
    "        driver = webdriver.Chrome(r'C:\\Users\\David\\chromedriver_win32\\chromedriver.exe')\n",
    "        scroll_pause_time = 5\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        source_code = driver.page_source\n",
    "        soup = BeautifulSoup(source_code, 'html.parser')\n",
    "        \n",
    "            ######################################### basic section\n",
    "        # top 10\n",
    "        iii = []\n",
    "        bb = []\n",
    "        outsidenav = soup.find_all(\"div\", class_=['SectionWithOutSideNavigationCarousel__Content-laj44i-0', 'cTNIGb'])\n",
    "        for do in outsidenav:\n",
    "            d = do.find_all('div', class_ =[\"OutSideNavigationCarousel2__Content-lxp9vi-0\",\"fhZyLW\"])\n",
    "            for i in d:\n",
    "                ii = i.find_all('a', [\"ProductCardfragment__HoverStyledLink-jvv2ql-0\", \"lmeJlb\"])\n",
    "                for ai in ii:\n",
    "                    if base+ai.attrs['href'] not in bb:\n",
    "                        if count < 10 and url == \"https://class101.net/creative\":\n",
    "                            bb.append([\"creative\",\"Top10\", base+ai.attrs['href']])\n",
    "                        elif count >= 10 and url == \"https://class101.net/creative\":\n",
    "                            bb.append([\"creative\",\"New\" ,base+ai.attrs['href']])\n",
    "                        elif count < 10 and url == \"https://class101.net/money\":\n",
    "                            bb.append([\"money\",\"Top10\", base+ai.attrs['href']])\n",
    "                        elif count >= 10 and url == \"https://class101.net/money\":\n",
    "                            bb.append([\"money\",\"New\" ,base+ai.attrs['href']])\n",
    "\n",
    "                        elif count < 10 and url == \"https://class101.net/career\":\n",
    "                            bb.append([\"career\",\"Top10\", base+ai.attrs['href']])\n",
    "                        elif count >= 10 and url == \"https://class101.net/career\":\n",
    "                            bb.append([\"career\",\"New\" ,base+ai.attrs['href']])\n",
    "\n",
    "                        elif count < 10 and url == \"https://class101.net/kids\":\n",
    "                            bb.append([\"kids\",\"Top10\" ,base+ai.attrs['href']])\n",
    "                        elif count >= 10 and url == \"https://class101.net/kids\":\n",
    "                            bb.append([\"kids\",\"New\" ,base+ai.attrs['href']])\n",
    "\n",
    "                        elif count < 10 and url == \"https://class101.net/signature\":\n",
    "                            bb.append([\"signature\",\"Top10\", base+ai.attrs['href']])\n",
    "                        elif count >= 10 and url == \"https://class101.net/signature\":\n",
    "                            bb.append([\"signature\",\"New\" ,base+ai.attrs['href']])\n",
    "                    count+=1\n",
    "        a.append(bb)\n",
    "        \n",
    "        \n",
    "        #######################################################\n",
    "        #######################################################\n",
    "        href = []\n",
    "        while True:\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "            # Wait to load page\n",
    "\n",
    "            source_code = driver.page_source\n",
    "            soup = BeautifulSoup(source_code, 'html.parser')\n",
    "            infinitediv = soup.find_all(\"div\", class_=['SectionWithInfiniteScrollGridList__Content-sc-1b8v326-0', 'exzfmE'])\n",
    "            infItems = []\n",
    "\n",
    "            for y in infinitediv:\n",
    "                infItems.append(y.find_all('a', class_ = ['ProductCardWithLastUpdatedInformationfragment__HoverStyledLink-sc-146ouht-0', 'hVwDSX']))\n",
    "\n",
    "            for yy in infItems:\n",
    "                for yyy in yy:\n",
    "                    if len(href) < 100:\n",
    "                        if base+yyy.attrs['href'] not in href:\n",
    "                            if url == \"https://class101.net/creative\":   \n",
    "                                href.append([\"creative\",base+yyy.attrs['href']])\n",
    "                            elif url == \"https://class101.net/money\":   \n",
    "                                href.append([\"money\",base+yyy.attrs['href']])\n",
    "                            elif url == \"https://class101.net/career\":   \n",
    "                                href.append([\"career\",base+yyy.attrs['href']])\n",
    "                            elif url == \"https://class101.net/kids\":   \n",
    "                                href.append([\"kids\",base+yyy.attrs['href']])\n",
    "                            elif url == \"https://class101.net/signature\":   \n",
    "                                href.append([\"signature\",base+yyy.attrs['href']])\n",
    "                            \n",
    "\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            if (len(href) > 100) or (new_height == last_height):\n",
    "                driver.quit()\n",
    "                break\n",
    "            last_height = new_height  \n",
    "        all_links.append(href)\n",
    "        return\n",
    "    \n",
    "    #####whole function\n",
    "    scroll(url)\n",
    "\n",
    "ne = []\n",
    "for i in a:\n",
    "    for j in i:\n",
    "        ne.append(j)\n",
    "n = pd.DataFrame(ne)\n",
    "\n",
    "n.columns = [\"0\",\"1\",\"2\"]\n",
    "\n",
    "final_links = []\n",
    "for i in all_links:\n",
    "    for j in i:\n",
    "        final_links.append(j) \n",
    "        \n",
    "linkdf = pd.DataFrame(final_links)  \n",
    "linkdf.insert(1, \"sub-category\", \" \") \n",
    "linkdf.columns = [\"0\",\"1\",\"2\"]\n",
    "linkdf.to_csv(\"alllinks.csv\")\n",
    "\n",
    "frame = [n, linkdf]\n",
    "test = pd.concat(frame, axis = 0, join=\"outer\",ignore_index=True)\n",
    "test.to_csv(\"testdf.csv\")\n",
    "\n",
    "######################################\n",
    "######################################\n",
    "######################################\n",
    "\n",
    "al = pd.read_csv(\"testdf.csv\")\n",
    "al = al.drop(['Unnamed: 0'], axis = 1)\n",
    "nlink = list(al[\"2\"])\n",
    "\n",
    "all_item = []\n",
    "for i in range(len(nlink)):\n",
    "    url = nlink[i]\n",
    "    item = []\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\David\\chromedriver_win32\\chromedriver.exe')\n",
    "    \n",
    "    def getdata(url):\n",
    "        print(url)\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Wait to load page\n",
    "        print('while...')\n",
    "\n",
    "        source_code = driver.page_source\n",
    "        soup = BeautifulSoup(source_code, 'html.parser')\n",
    "\n",
    "        initial = soup.find_all(\"div\", class_=['sc-dkiTmt', 'cQiUWe'])\n",
    "        for y in initial:\n",
    "            initialdiv = y.find_all('div', class_ = ['ProductHeader__Header-az1cmu-0 leqDJG','AffixProductHeader__StyledProductHeader-sc-1l73jdk-1','idfLTb'])\n",
    "            for p in initialdiv:              \n",
    "                top2 = p.find_all('div', class_ = ['sc-bdnylx','gGpzAi','ProductHeader__Tag-az1cmu-1','iOzkLq'])\n",
    "                for k in top2:\n",
    "                    if k.text not in item:\n",
    "                        item.append(k.text)\n",
    "\n",
    "                heading = p.find_all('h2', class_ = ['sc-bdnylx','cOMiUB','ProductHeader__Title-az1cmu-2','fkqBP'])\n",
    "                for j in heading:\n",
    "                    if j.text not in item:\n",
    "                        item.append(j.text)\n",
    "\n",
    "                recommend = p.find_all('div', class_=\"SalesProductInfoTable__Row-sc-1uyx5v1-1 gOzdaG\")\n",
    "                for r in recommend:\n",
    "                    t2 = r.find_all('div', class_ = ['SalesProductInfoTable__ButtonContainer-sc-1uyx5v1-5','jOtFPR','SalesProductInfoTable__StyledExperiment-sc-1uyx5v1-4','diAwWT'])\n",
    "                    for t in t2:\n",
    "                        each = t.find_all('button', class_ = [\"sc-ksluoS\", \"dyNqkJ\" ,\"sc-kfYqjs\", \"kpPRTM\",\"SalesProductInfoTable__WishlistButton-sc-1uyx5v1-2\", \"fxUQsM\"])\n",
    "                        for e in each:\n",
    "                            if e.text not in item:\n",
    "                                item.append(e.text)                               \n",
    "        \n",
    "        while True:\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            source_code = driver.page_source\n",
    "            soup = BeautifulSoup(source_code, 'html.parser')\n",
    "            \n",
    "            ########left section\n",
    "            initial = soup.find_all(\"div\", class_=['sc-dkiTmt', 'cQiUWe'])\n",
    "            for y in initial:\n",
    "                left = y.find_all('div', class_ = ['sc-eiQXzm','cSojtf', 'commonStyles__StaticPositionedColumn-sc-6jqw77-1', 'bvouQZ'])\n",
    "                for l in left:\n",
    "                    pro = l.find_all('div', class_ = ['commonStyled__ContentArea-sc-17rimc7-0','ACCCm'])\n",
    "                    for pr in pro:\n",
    "                        p = pr.find_all('div', class_ = ['sc-dkiTmt','cQiUWe'])\n",
    "\n",
    "                        #pro heading section\n",
    "                        for i in p:\n",
    "                            pi = i.find_all('div', class_ = ['CreatorIntroSection__Title-sc-18c4e1x-0','bdBflQ'])\n",
    "                            for p in pi:\n",
    "                                #name section\n",
    "                                pname = p.find_all('div', class_ = ['sc-bdnylx','jckokO',  'ContentSectionStyle__SectionTitle-sc-1oywcqb-1', \"CreatorIntroSection__WordBrokenSectionTitle-sc-18c4e1x-1\",\"iJQLEl\" ,\"bSBRTB\"])\n",
    "                                for pn in pname:\n",
    "                                    if pn not in item:\n",
    "                                        item.append(pn)\n",
    "\n",
    "                                #link section\n",
    "                                plink = p.find_all('div', class_ = ['ChannelButtonGroup__Container-sc-1yqg9fu-0','zUbfY'])\n",
    "                                for pl in plink:\n",
    "                                    pm = pl.find_all('a', class_ = ['sc-hcmsuR','ARwUf','sc-dlnjPT','sc-fFSRdu','cuIYFB','goHICw'])\n",
    "                                    for a in pm:\n",
    "                                        if a.attrs['href'] not in item:\n",
    "                                            item.append(a.attrs['href'])\n",
    "                        \n",
    "                    #pro info section\n",
    "                    for j in l:  \n",
    "                        pp = j.find_all('div', class_ = ['ContentSectionStyle__SectionBodyColumn-sc-1oywcqb-3','cNvNAk'])\n",
    "                        for jj in pp:\n",
    "                            mm = jj.find_all('div', class_ = ['FoldableContent__Container-eh1nn2-0','lmAXPf'])\n",
    "                            for j in mm:\n",
    "                                nn = j.find_all('div', class_ = ['FoldableContent__ViewContainer-eh1nn2-1','jPwmCe'])\n",
    "                                for jj in nn:\n",
    "                                    i = jj.find_all('div', class_ = ['FoldableContent__InnerContainer-eh1nn2-2','ionrXH'])\n",
    "                                    for pp in i:\n",
    "                                        co = pp.find_all('div', class_ = ['SanitizeHtml__SanitizeHtmlContainer-ldtmln-0','gNRtXt'])\n",
    "                                        for c in co:\n",
    "                                            coo = c.find_all('ul')\n",
    "                                            for nm in coo:\n",
    "                                                if nm.text not in item:\n",
    "                                                    item.append(nm.text)\n",
    "\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            if new_height == last_height:\n",
    "                driver.quit()\n",
    "                break\n",
    "            last_height = new_height\n",
    "        return\n",
    "    getdata(url)\n",
    "    all_item.append(item)\n",
    "dd = pd.DataFrame(all_item)\n",
    "\n",
    "frames = [test, dd]\n",
    "data = pd.concat(frames, axis = 1)\n",
    "data.to_csv(\"DATA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-credits",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-woman",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
